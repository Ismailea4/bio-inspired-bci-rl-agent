# test_full_integration.py
print("ğŸ”— TEST D'INTÃ‰GRATION COMPLET\n" + "="*40)

from envs.gambling_task import GamblingTaskEnv
from models.rpe_agent import ActorCriticAgent
import numpy as np

# Configuration
N_TRIALS = 200  # Test rÃ©duit pour la validation
SEED = 42

# Initialisation
env = GamblingTaskEnv(p_left=0.4, p_right=0.6, reversal_trial=100, seed=SEED)
agent = ActorCriticAgent(state_size=10, action_size=2, learning_rate=0.05, gamma=0.99)

# ğŸ”¥ CORRECTION: env.reset() retourne 1 valeur seulement
obs = env.reset()

print(f"ğŸ“Š Environnement: {N_TRIALS} trials, reversal @ trial 100")
print(f"ğŸ§  Agent: lr=0.05, gamma=0.99\n")

# Boucle d'entraÃ®nement
rewards = []
rpes = []

for trial in range(N_TRIALS):
    action = agent.get_action(obs, deterministic=False)
    obs_next, reward, done, info = env.step(action)
    delta = agent.update(obs, action, reward, obs_next, done)
    
    rewards.append(reward)
    rpes.append(delta)
    obs = obs_next
    
    if (trial + 1) % 20 == 0:
        print(f"Trial {trial+1:3d} | Action: {action} | Reward: {reward} | RPE: {delta:+6.3f}")

# RÃ©sultats
print("\n" + "="*40)
print("ğŸ“ˆ RÃ‰SULTATS FIN")
print(f"RÃ©compense totale: {np.sum(rewards)} / {N_TRIALS}")
print(f"RPE moyen: {np.mean(rpes):+.4f}")
print(f"DerniÃ¨re politique: {agent.policy_history[-1]}")

# VÃ©rification apprentissage
pre = np.mean(rewards[:20])
post = np.mean(rewards[-20:])
print(f"\nTaux rÃ©ussite dÃ©but: {pre:.1%}")
print(f"Taux rÃ©ussite fin: {post:.1%}")

if post > pre + 0.1:
    print("\nâœ… L'AGENT APPREND CORRECTEMENT!")
else:
    print("\nâš ï¸ VÃ©rifie les hyperparamÃ¨tres (lr, gamma)")

print("\nğŸ‰ TEST D'INTÃ‰GRATION RÃ‰USSI!")