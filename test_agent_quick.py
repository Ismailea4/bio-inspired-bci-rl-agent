# test_agent_quick.py
print("ğŸ§  DÃ‰MARRAGE DU TEST DE L'AGENT...")

try:
    from rpe_agent import ActorCriticAgent, softmax
    import numpy as np
    
    print("âœ… Import rÃ©ussi")
    
    # CrÃ©er l'agent
    agent = ActorCriticAgent(state_size=10, action_size=2, learning_rate=0.01)
    print("âœ… Agent crÃ©Ã©")
    
    # Test softmax
    logits = np.array([0.5, -0.5])
    probs = softmax(logits)
    print(f"âœ… Softmax test: {probs} (sum={np.sum(probs):.2f})")
    
    # Test action selection
    state = np.random.randn(10)
    action = agent.get_action(state, deterministic=False)
    print(f"âœ… Action sÃ©lectionnÃ©e: {action}")
    
    # Test RPE computation
    next_state = np.random.randn(10)
    delta = agent.compute_rpe(state, action, reward=1.0, state_next=next_state, done=False)
    print(f"âœ… RPE calculÃ©: Î´ = {delta:.4f}")
    
    # Test update
    delta_updated = agent.update(state, action, reward=1.0, state_next=next_state, done=False)
    print(f"âœ… Agent mis Ã  jour, nouveau Î´: {delta_updated:.4f}")
    
    print(f"\nğŸ“Š Historique RPE: {len(agent.rpe_history)} valeurs")
    print(f"ğŸ“Š DerniÃ¨re V(s): {agent.value_history[-1]:.4f}")
    print("\nğŸ‰ AGENT TESTÃ‰ AVEC SUCCÃˆS!")
    
except Exception as e:
    print(f"\nâŒ ERREUR: {e}")
    import traceback
    traceback.print_exc()
